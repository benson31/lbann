# LBANN Python Interface

This provides a convenient Python wrapper for configuring and running
LBANN experiments. The syntax is meant to be deliberately reminiscent
of [PyTorch](https://pytorch.org/).

This is still a work in progress, so please [open an
issue](https://github.com/LLNL/lbann/issues/new) if you find any
problems or have feature suggestions.

* For more details about the LBANN/ONNX converter,
see [here](docs/onnx/README.md).
* For more details about the *accuracy/loss* visualization script
(also known as `lbplot`), see [here](docs/plot/README.md).

## Setup

The `lbann` Python package is installed as part of the LBANN build
process. Usage instructions depend on which build method was used.

_Spack_: `module load lbann`.

_CMake_: The Python package is typically installed inside the install
directory at `<install directory>/share/python`. To make sure Python
can detect it, update the `PYTHONPATH` environment variable:
```sh
export PYTHONPATH=<install directory>/share/python:${PYTHONPATH}
```
Alternatively, the package can be installed into a Python
site-packages directory so that Python can detect it immediately. This
usually requires an active virtual environment or root access. To
build with this approach, pass `-DLBANN_PYTHON_IN_INSTALL_DIR=OFF` as
an argument into CMake during the build process.

_Warnings_:
* The build system is still under active development.
* Python 2 is not supported.
* The CMake build process does not handle package dependencies. See
  `$LBANN_HOME/cmake/configure_files/setup.py.in` for the full list of
  dependencies.
* Installing the ONNX Python package may require some work. See [the
  documentation](https://github.com/onnx/onnx#source).
  * If you do not already have the ONNX Python package installed, you
    will need to ensure the `protoc` compiler is in your path when you
    run this. Either load the appropriate Spack module or add
    `<install directory>/bin` to `$PATH` before running.

## Modules

### `lbann`

The `Model` class describes a neural network model and contains the
following components:

* A `Layer` is a tensor operation, arranged within a directed acyclic
  graph. A layer will recieve input tensors from its parents and will
  send output tensor to its children. Once the layer graph has been
  constructed, it may be helpful to call `traverse_layer_graph`, which
  is a generator function that traverses the layer graph in a
  topological order.
* A `Weights` is a set of trainable parameters, typically associated
  with one or more layers. The initial values are populated with an
  `Initializer` and it is optimized with an `Optimizer`.
* The `ObjectiveFunction` is a mathematical expression that the
  optimization algorithm will attempt to minimize. It is made up of
  multiple `ObjectiveFunctionTerm`s, which are added up (possibly with
  scaling factors) to obtain the full objective function. There are
  currently two objective function terms:
    - `LayerTerm` gets its value from a `Layer`. The layer must output
      a scalar (tensor with one entry).
    - `L2WeightRegularization` gets its value by computing the L2 norm
      of the model weights.
* A `Metric` reports values to the user, which is helpful for
  evaluating the progress of training. They get the their values from
  layers, which must output scalars (tensors with one entry).
* A `Callback` performs some function at various points during
  training. They are helpful for performing advanced training
  techniques.

Many of these components, e.g. layers, are automatically generated by
parsing messages defined in `src/proto/lbann.proto`. This file is
currently the best source for documentation. Note that LBANN currently
only supports static models, i.e. models with static execution graphs.

### `lbann.proto`

The `save_prototext` function can be used to export an LBANN
experiment to a prototext file. A typical experiment is comprised of a
model, data reader, and optimizer.

### `lbann.modules`

This is a collection of neural network modules, which are patterns of
layers that take an input layer to produce an output layer. Once
created, a `Module` is _callable_. Calling it with an input layer will
add the module's pattern to the layer graph and will return the output
layer.

_A possible note of confusion_: "modules" in LBANN are similar to
"layers" in PyTorch, TensorFlow, and Keras. LBANN uses "layer" in a
similar manner as Caffe.

### `lbann.models`

This consists of common and influential neural network models. They
are implemented as `Module`s and can be used as components within more
complicated models.

### `lbann.launcher`

The `run` function interfaces with job schedulers on HPC clusters. It
will either submit a batch job (if on a login node) or run with an
existing node allocation (if on a compute node).

_LLNL users_: The `run` function in the `lbann.contrib.lc.launcher`
module provides similar functionality, with defaults and optimizations
for LC systems.

### `lbann.onnx`

This contains functionality to convert between LBANN and ONNX models.

## Examples

A simple (and not very good) convolutional neural network for MNIST
data:

```py
import lbann
import lbann.proto

# ----------------------------------------------------------
# Construct layer graph
# ----------------------------------------------------------
# Note: The first argument to every layer specifies its parents,
# i.e. the sources for its input tensors.

# Input data
# Note: Order matters for the children of the input layer!
input = lbann.Input()           # Interacts with data reader
images = lbann.Identity(input)  # NCHW image tensor
labels = lbann.Identity(input)  # One-hot vector

# Simple convolutional network
conv = lbann.Convolution(
    images,
    num_dims=2,             # 2D convolution for NCHW tensors
    num_output_channels=64, # I.e. number of filters
    conv_dims_i=5,          # Convolution window size (64x3x5x5 kernel)
    conv_pads_i=2,          # Padding of 2 in every dimension
    conv_strides_i=2,       # Stride of 2 in every dimension
    has_bias=True)          # Channel-wise bias
bn = lbann.BatchNormalization(conv)
relu = lbann.Relu(bn)
pool = lbann.Pooling(
    relu,
    num_dims=2,         # 2D pooling (for NCHW tensors)
    pool_dims_i=3,      # 3x3 pooling window
    pool_pads_i=1,      # Padding of 1 in every dimension
    pool_strides_i=2,   # Stride of 2 in every dimension
    pool_mode='max')    # Max pooling
fc = lbann.FullyConnected(
    pool,
    num_neurons=10, # Output size
    has_bias=False) # Entry-wise bias
softmax = lbann.Softmax(fc)

# Compute values for objective function and metrics
loss = lbann.CrossEntropy([softmax, labels])
acc = lbann.CategoricalAccuracy([softmax, labels])

# ----------------------------------------------------------
# Construct model
# ----------------------------------------------------------

mini_batch_size = 256
num_epochs = 10
obj = lbann.ObjectiveFunction([loss])
metrics = [lbann.Metric(acc, name='accuracy', unit='%')]
callbacks = [
    lbann.CallbackPrint(), # Print basic information
    lbann.CallbackTimer()  # Print timing information
]
model = lbann.Model(
    mini_batch_size, num_epochs,
    layers=lbann.traverse_layer_graph(input),   # Layers connected to input
    objective_function=obj,
    metrics=metrics,
    callbacks=callbacks)

# ----------------------------------------------------------
# Save the model to a prototext file.
# ----------------------------------------------------------

lbann.proto.save_prototext('test.prototext', model=model)

```

See the implementation of LeNet in
`$LBANN_HOME/model_zoo/vision/lenet.py` for a more comprehensive
example.
